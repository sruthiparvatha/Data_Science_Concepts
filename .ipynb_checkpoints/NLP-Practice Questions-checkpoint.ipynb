{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b648c8c-6bcf-455f-b57f-241e0bd501ac",
   "metadata": {},
   "source": [
    "# Tokenization in NLP\n",
    ".\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b2216-68a7-4cdf-8ffe-72aae4c4784d",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down a text into smaller units called \"tokens.\" These tokens can be words, subwords,5 phrases, or even individual characters, depending on the specific task and tokenizer used. It's often the first step in an NLP pipeline, as most NLP models require input in tokenized form.\n",
    "<br/>\n",
    "Types of Tokenization:\n",
    "•\tWord Tokenization: Splits text into individual words.\n",
    "•\tSentence Tokenization: Splits text into individual sentences.6\n",
    "•\tSubword Tokenization (e.g., WordPiece, BPE): Breaks words into smaller, more common subword units, especially useful for handling out-of-vocabulary words and morphologically rich languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f19b2d-e827-44f5-8b02-6b5b5c4dc352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ee2bc7-5d2c-40c6-a354-c72c90f67e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens:  ['Artificial', 'Intelligence', 'is', 'developing', 'at', 'a', 'rapid', 'pace', '.', 'So', 'it', 'is', 'immensely', 'rewarding', 'to', 'work', 'in', 'an', 'emerging', 'and', 'challenging', 'space', '.']\n",
      "Sentence Tokens:  ['Artificial Intelligence is developing at a rapid pace.', 'So it is immensely rewarding to work in an emerging and challenging space.']\n",
      "Last Sentence:  So it is immensely rewarding to work in an emerging and challenging space.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Artificial Intelligence is developing at a rapid pace. So it is immensely rewarding to work in an emerging and challenging space.\"\n",
    "\n",
    "# Word Tokenization\n",
    "word_tokens = word_tokenize(text)\n",
    "print(\"Word Tokens: \", word_tokens)\n",
    "\n",
    "# Sentence Tokenization\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(\"Sentence Tokens: \", sentence_tokens)\n",
    "print(\"Last Sentence: \", sentence_tokens[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bcff4c-ee19-4c9b-8d94-aa0ab477f289",
   "metadata": {},
   "source": [
    "# Stemming and Lemmitization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d9408-b191-41c0-98a8-fa2031c07fdb",
   "metadata": {},
   "source": [
    "Both stemming and lemmatization are text normalization techniques used to reduce words to their base or root form. The main difference lies in their approach and the quality of the output:\n",
    "•\tStemming: A more crude, rule-based process that chops off suffixes from words, often resulting in words that are not actual dictionary words. It's faster but can be less accurate.\n",
    "o\tExample: \"running\" -> \"run\", \"caring\" -> \"car\" (incorrect)\n",
    "•\tLemmatization: A more sophisticated process that uses vocabulary and morphological analysis of words to return their dictionary base form (lemma). It's slower but more accurate as it considers the word's meaning and context.\n",
    "o\tExample: \"running\" -> \"run\", \"caring\" -> \"care\", \"better\" -> \"good\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28751a8a-978f-4ebd-9cf4-bfa3d6d6fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using NLTK for stemming and Spacy for Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b2ed5c-dc34-49e1-ac75-911e8334d19a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (2.11.5)\n",
      "Requirement already satisfied: jinja2 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\practice_ds\\nlp\\nlp\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b19e2b1f-ffc0-4c90-b0d8-3fea4340a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "634c72bd-33ab-4c93-b117-4775349cfabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt') #for tokenization\n",
    "# nltk.download('wordnet') #for lemmatization\n",
    "# nltk.download('omw-1.4') # Open Multilingual WordNet (version 1.4) provides support for WordNet in multiple languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0756c5a5-c50d-465e-9665-409e547c9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SpaCy English model\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a35db9fb-8e5b-4713-8a86-988bdd5d9087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed words:  ['run', 'run', 'ran', 'fli', 'flew', 'care', 'car']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "words = [\"running\",\"runs\", \"ran\",\"flying\",\"flew\",\"caring\",\"cars\"]\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "print(\"Stemmed words: \", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0b523c8-61f6-4c90-ab9a-d55a8d2c4708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words (NLTK): ['run', 'run', 'ran', 'fly', 'flew', 'care', 'car']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization (using NLTK's WordNetLemmatizer, which requires POS tag for better accuracy)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# A simple function to get Part of Speech (POS) tag for lemmatizer\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        \"J\" : wordnet.ADJ,\n",
    "        \"N\" : wordnet.NOUN,\n",
    "        \"V\" : wordnet.VERB,\n",
    "        \"R\" : wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN) # Default to Noun if not found\n",
    "\n",
    "lemmatized_words_nltk = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "print(\"Lemmatized Words (NLTK):\", lemmatized_words_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c5e5c4d-f140-4eee-bddc-3576400fd7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words (Spacy) : he be run very fast and care for his family .\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization (using SpaCy, which is generally more robust)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text_for_lemmitization = \"He was running very fast and caring for his family.\"\n",
    "doc = nlp(text_for_lemmitization)\n",
    "lemmatized_words_spacy = [token.lemma_ for token in doc]\n",
    "print(\"Lemmatized words (Spacy) :\", \" \".join(lemmatized_words_spacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf4821-3060-4d50-9438-faa86dbd21b8",
   "metadata": {},
   "source": [
    "# Removal of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7328edd8-4813-49df-b886-7c7af95775e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words:  ['This', 'is', 'a', 'very', 'good', 'example', 'to', 'show', 'how', 'stop', 'words', 'are', 'removed', 'from', 'a', 'text', '.']\n",
      "Filtered Words (after removing stop words):  ['good', 'example', 'show', 'stop', 'words', 'removed', 'text', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords') # Download stopwords\n",
    "\n",
    "text = \"This is a very good example to show how stop words are removed from a text.\"\n",
    "stop_words = set(stopwords.words('english')) #set data structure ensures duplicate values cannot be stored\n",
    "# returns a collection of stop words\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "filtered_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Original Words: \", word_tokens)\n",
    "print(\"Filtered Words (after removing stop words): \", filtered_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543eec5-b3e1-4ab2-952e-e611e0b8fa0c",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9cdfd-5978-4272-877b-5b0b86797b04",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects how important a word is to a document in a collection or corpus.7 It is widely used in information retrieval and text mining.\n",
    "* TF-IDF Score: The product of TF and IDF. A high TF-IDF score indicates that a word is frequent in a specific document but rare across the entire corpus, suggesting it's a good descriptor for that document.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d29544f9-fc44-4fb8-b502-a3057ff0cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b82d370a-899f-4120-ad80-c9e4bfb2a1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix: \n",
      "[[0.         0.         0.31502724 0.24464675 0.41422296 0.\n",
      "  0.         0.41422296 0.24464675 0.         0.31502724 0.31502724\n",
      "  0.4892935 ]\n",
      " [0.46499651 0.         0.         0.27463443 0.         0.\n",
      "  0.46499651 0.         0.27463443 0.46499651 0.35364183 0.\n",
      "  0.27463443]\n",
      " [0.         0.48775955 0.37095371 0.28807865 0.         0.48775955\n",
      "  0.         0.         0.28807865 0.         0.         0.37095371\n",
      "  0.28807865]]\n",
      "\n",
      "TF-IDF scores for the first document: \n",
      "[('the', np.float64(0.4892935045993933)), ('fox', np.float64(0.4142229588893787)), ('jumps', np.float64(0.4142229588893787)), ('brown', np.float64(0.31502723701987084)), ('over', np.float64(0.31502723701987084)), ('quick', np.float64(0.31502723701987084)), ('dog', np.float64(0.24464675229969665)), ('lazy', np.float64(0.24464675229969665)), ('again', np.float64(0.0)), ('and', np.float64(0.0)), ('is', np.float64(0.0)), ('jump', np.float64(0.0)), ('never', np.float64(0.0))]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Never jump over the lazy dog again.\",\n",
    "    \"The brown dog is quick and lazy.\"\n",
    "]\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "print(\"TF-IDF Matrix: \")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Map features to their TF-IDF scores for the first document\n",
    "print(\"\\nTF-IDF scores for the first document: \")\n",
    "doc1_tfidf_scores = dict(zip(feature_names, tfidf_matrix.toarray()[0]))\n",
    "print(sorted(doc1_tfidf_scores.items(), key=lambda x:x[1], reverse = True)) # sort a dictionary based on values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b064066-c670-4399-b046-900638c07210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bed657-b001-4fa1-82bb-c1ea121abae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87321e95-2e5e-4301-b808-5224420a0e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
